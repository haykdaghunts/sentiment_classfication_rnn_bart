{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of IMBD Movie Ratings with RNN \n",
    "\n",
    "The notebook includes text preprocessing along with the embedding part and the training using customly written LSTM neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../IMDB-Dataset.csv\" # path to a csv file downloaded form https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing and Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at our dataset that is currently in a CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading csv data\n",
    "data_path = root\n",
    "df = pd.read_csv(data_path)\n",
    "df['sentiment'] = df['sentiment'].apply(lambda x:1 if x == 'positive' else 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use in RNN tasks we need to tokenize our dataset and bring it into compatible format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [s.lower() for s in re.split(r'\\W+', text) if s]\n",
    "\n",
    "\n",
    "# Maximum tokens allowed per review.\n",
    "max_tokens = 80\n",
    "\n",
    "# Defining set of stopwords to remove from the vocabulary and token lists.\n",
    "stop_words = {\"a\", \"an\", \"and\", \"the\"}\n",
    "\n",
    "\n",
    "# Building the vocabulary using training data.\n",
    "freqs = Counter()\n",
    "for text in train_df['review']:\n",
    "    tokens = [token for token in tokenize(text) if token not in stop_words][:max_tokens]\n",
    "    freqs.update(tokens)\n",
    "\n",
    "# Initializing the vocabulary with special tokens.\n",
    "vocab = {'<eos>': 0, '<unk>': 1}\n",
    "# Adding the 50 most common tokens from the training data.\n",
    "for token, _ in freqs.most_common(50):\n",
    "    vocab[token] = len(vocab)\n",
    "\n",
    "\n",
    "# Mapping Tokens to unique indices\n",
    "# if token does not exist in vocabulary we assign it to <unk> token\n",
    "def tokens_to_indices(tokens, vocab):\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "\n",
    "# Preparing data: Creating tuples (raw_text, tokens, token_indices, sentiment).\n",
    "def prepare_data(df, vocab, max_tokens=40):\n",
    "    data_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        raw_text = row['review']\n",
    "        sentiment = row['sentiment']\n",
    "        tokens = tokenize(raw_text)[:max_tokens]  # truncating tokens\n",
    "        indices = tokens_to_indices(tokens, vocab)\n",
    "        data_list.append((raw_text, tokens, indices, sentiment))\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'of': 86474,\n",
       "         'to': 74070,\n",
       "         'i': 69669,\n",
       "         'is': 64651,\n",
       "         'it': 61590,\n",
       "         'this': 54924,\n",
       "         'in': 53014,\n",
       "         'br': 48456,\n",
       "         'that': 40760,\n",
       "         'was': 35086,\n",
       "         'movie': 33781,\n",
       "         's': 33139,\n",
       "         'film': 25914,\n",
       "         'as': 24935,\n",
       "         'with': 24597,\n",
       "         'for': 23975,\n",
       "         'but': 23867,\n",
       "         'on': 20404,\n",
       "         't': 19048,\n",
       "         'you': 18101,\n",
       "         'not': 17667,\n",
       "         'have': 17258,\n",
       "         ...})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs # Most frequent words in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = prepare_data(train_df, vocab, max_tokens)\n",
    "val_data = prepare_data(val_df, vocab, max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's Define our Dataset and Dataloader classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            data: list of tuples (raw_text, tokens, token_indices, sentiment)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        # Sorting by token list length (largest first)\n",
    "        self.data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            i: an integer index\n",
    "        Outputs:\n",
    "            data: A dictionary of {data, label}\n",
    "        \"\"\"\n",
    "        _, _, indices, sentiment = self.data[i]\n",
    "        return {\n",
    "            'data': torch.tensor(indices).long(),\n",
    "            'label': torch.tensor(sentiment).float()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataset objects.\n",
    "train_dataset = SentimentDataset(train_data)\n",
    "val_dataset = SentimentDataset(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the datastet: 40000\n",
      "Sample:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data': tensor([10, 13, 41,  4,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 10,\n",
       "          1,  1,  1,  1,  1,  1,  1,  1,  1, 49, 21,  1,  2,  1, 24,  1,  1, 33,\n",
       "         23, 32,  1,  1, 10,  6,  5,  1,  1,  3,  1, 41,  1,  3,  1, 36, 25, 40,\n",
       "          1,  1,  1, 17,  1,  1,  3,  1, 27,  1,  1, 19,  1,  1, 10, 38,  1,  1,\n",
       "          1,  1,  8,  1,  1,  1, 19,  1]),\n",
       " 'label': tensor(0.)}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Length of the datastet: {len(train_dataset)}')\n",
    "print('Sample:')\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a collate function for DataLoader to correctly handle batches.\n",
    "def collate(batch):\n",
    "    data = pad_sequence([item['data'] for item in batch])\n",
    "    lengths = torch.tensor([len(item['data']) for item in batch])\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "    return {\n",
    "        'data': data,\n",
    "        'lengths': lengths,\n",
    "        'label': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': tensor([[ 1,  1, 41,  ...,  1,  1,  1],\n",
       "         [ 1,  1,  1,  ...,  1,  1,  1],\n",
       "         [ 1,  1,  1,  ...,  1,  1, 38],\n",
       "         ...,\n",
       "         [49,  1,  4,  ..., 36, 36,  1],\n",
       "         [ 1, 38,  1,  ...,  1,  1,  1],\n",
       "         [ 1, 47,  1,  ...,  3,  1, 14]]),\n",
       " 'lengths': tensor([80, 80, 80, 80, 60, 80, 80, 80, 80, 80, 55, 80, 57, 80, 80, 80]),\n",
       " 'label': tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0.])}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training we will use our <code>RNNClassifier</code> that uses the output of last hidden layer of lstm to predict the label.\n",
    "Have a look at the <code>RNNClassfier</code> class in <code>classfier.py</code> to get better understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since RNNClassifier uses LSTM and Embedding, check  <code>Embedding</code> and <code>LSTM</code> classes in <code>layers.py</code> file to completely understand the implementation part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifier import RNNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def compute_accuracy(model, data_loader):\n",
    "    \"\"\"Computes the accuracy of the model\"\"\"\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    device = next(model.parameters())\n",
    "    \n",
    "    for i, x in enumerate(data_loader):\n",
    "        input = x['data']\n",
    "        lengths = x['lengths']\n",
    "        label = x['label']\n",
    "        pred = model(input, lengths)\n",
    "        corrects += ((pred > 0.5) == label).sum().item()\n",
    "        total += label.numel()\n",
    "        \n",
    "        if i > 0  and i % 100 == 0:\n",
    "            print('Step {} / {}'.format(i, len(data_loader)))\n",
    "    \n",
    "    return corrects / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNClassifier(num_embeddings=len(vocab), embedding_dim=20, hidden_size=32)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's define the training function in accordance with pytorch's training pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/10]: 100%|██████████| 2500/2500 [01:19<00:00, 31.51it/s, loss=0.658]\n",
      "Validation Epoch [1/10]: 100%|██████████| 625/625 [00:05<00:00, 110.67it/s, loss=0.669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6848, Val Loss: 0.6680, Accuracy: 59.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [2/10]: 100%|██████████| 2500/2500 [01:21<00:00, 30.66it/s, loss=0.565]\n",
      "Validation Epoch [2/10]: 100%|██████████| 625/625 [00:05<00:00, 112.43it/s, loss=0.672]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 0.6631, Val Loss: 0.6566, Accuracy: 61.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [3/10]: 100%|██████████| 2500/2500 [01:18<00:00, 31.75it/s, loss=0.679]\n",
      "Validation Epoch [3/10]: 100%|██████████| 625/625 [00:05<00:00, 116.54it/s, loss=0.693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 0.6507, Val Loss: 0.6526, Accuracy: 61.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [4/10]: 100%|██████████| 2500/2500 [01:18<00:00, 31.74it/s, loss=0.772]\n",
      "Validation Epoch [4/10]: 100%|██████████| 625/625 [00:06<00:00, 102.00it/s, loss=0.689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 0.6426, Val Loss: 0.6451, Accuracy: 62.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [5/10]: 100%|██████████| 2500/2500 [01:20<00:00, 30.92it/s, loss=0.627]\n",
      "Validation Epoch [5/10]: 100%|██████████| 625/625 [00:05<00:00, 114.52it/s, loss=0.704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 0.6343, Val Loss: 0.6430, Accuracy: 62.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [6/10]: 100%|██████████| 2500/2500 [01:19<00:00, 31.37it/s, loss=0.561]\n",
      "Validation Epoch [6/10]: 100%|██████████| 625/625 [00:05<00:00, 111.63it/s, loss=0.718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 0.6280, Val Loss: 0.6384, Accuracy: 62.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [7/10]: 100%|██████████| 2500/2500 [01:17<00:00, 32.19it/s, loss=0.592]\n",
      "Validation Epoch [7/10]: 100%|██████████| 625/625 [00:05<00:00, 116.08it/s, loss=0.725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 0.6218, Val Loss: 0.6366, Accuracy: 63.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [8/10]: 100%|██████████| 2500/2500 [01:17<00:00, 32.35it/s, loss=0.53] \n",
      "Validation Epoch [8/10]: 100%|██████████| 625/625 [00:05<00:00, 123.77it/s, loss=0.705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 0.6156, Val Loss: 0.6378, Accuracy: 63.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [9/10]: 100%|██████████| 2500/2500 [01:14<00:00, 33.41it/s, loss=0.784]\n",
      "Validation Epoch [9/10]: 100%|██████████| 625/625 [00:05<00:00, 124.60it/s, loss=0.715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 0.6099, Val Loss: 0.6369, Accuracy: 63.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [10/10]: 100%|██████████| 2500/2500 [01:19<00:00, 31.58it/s, loss=0.521]\n",
      "Validation Epoch [10/10]: 100%|██████████| 625/625 [00:05<00:00, 115.27it/s, loss=0.713]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 0.6040, Val Loss: 0.6345, Accuracy: 63.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def initialize_training(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=10,\n",
    "    lr=0.001,\n",
    "    criterion=None,\n",
    "    optimizer_class=torch.optim.Adam\n",
    "):\n",
    "    \"\"\"\n",
    "    Initializes the training process and runs the training/validation loops.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model to be trained.\n",
    "        train_loader: DataLoader for training data (returns dict with keys ['data', 'label', 'lengths']).\n",
    "        val_loader: DataLoader for validation data (same structure as train_loader).\n",
    "        epochs: Number of epochs to train for.\n",
    "        lr: Learning rate for the optimizer.\n",
    "        criterion: Loss function to use (default: BCELoss).\n",
    "        optimizer_class: Optimizer class to use (default: Adam).\n",
    "    \"\"\"\n",
    "    if criterion is None:\n",
    "        criterion = nn.BCELoss()  # Default to binary cross-entropy loss\n",
    "\n",
    "    optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_loop = tqdm(train_loader, desc=f'Training Epoch [{epoch + 1}/{epochs}]')\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=epochs * len(train_loader) / 5, gamma=0.7)\n",
    "\n",
    "        \n",
    "        for batch in train_loop:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Extract data, labels, and lengths from batch\n",
    "            sequences = batch['data']  # Input sequences\n",
    "            labels = batch['label']    # Ground truth labels\n",
    "            lengths = batch['lengths'] # Actual lengths of sequences\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(sequences, lengths)  # Transpose sequences to (seq_len, batch_size)\n",
    "            \n",
    "            # Calculate loss and perform backpropagation\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "        scheduler.step()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loop = tqdm(val_loader, desc=f'Validation Epoch [{epoch + 1}/{epochs}]')\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loop:\n",
    "                sequences = batch['data']  # Input sequences\n",
    "                labels = batch['label']    # Ground truth labels\n",
    "                lengths = batch['lengths'] # Actual lengths of sequences\n",
    "\n",
    "                outputs = model(sequences, lengths)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                predictions = (outputs > 0.5).float()  # Threshold at 0.5\n",
    "                correct += (predictions == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                val_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = correct / total * 100\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "initialize_training(model=model, train_loader=train_loader, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we cna see model achieves around 64% accuracy which is a little bit better than random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `sentiment_analysis_with_bert.ipynb` we will increase the accuracy using pretrained transformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
